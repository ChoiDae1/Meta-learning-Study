{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmeta.datasets.helpers import omniglot\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MANN 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory 모듈 구현\n",
    "class Memory(nn.Module):\n",
    "    def __init__(self, size: int) -> None:\n",
    "        super(Memory, self).__init__()\n",
    "        self.size = size # 튜플형태 -> (N, M)\n",
    "        initial_state = torch.ones(self.size) * 1e-6 # [N, M]\n",
    "        self.register_buffer(\"initial_state\", initial_state.data)\n",
    "\n",
    "        self.initial_read  = nn.Parameter(torch.randn(1, self.size[1]) * 0.01) # [1, M]\n",
    "\n",
    "    def reset(self, batch_size: int) -> None:\n",
    "        self.matrix = self.initial_state.clone().repeat(batch_size, 1, 1) # [batch_size, N, M]\n",
    "    \n",
    "    def get_initial_read(self, batch_size: int) -> torch.Tensor:\n",
    "        return self.initial_read.clone().repeat(batch_size, 1) # [batch_size, M]\n",
    "    \n",
    "    def write(self, w: torch.Tensor, e: torch.Tensor, a: torch.Tensor) -> None:\n",
    "        self.matrix = self.matrix * (1 - torch.matmul(w.unsqueeze(-1), e.unsqueeze(1)))\n",
    "        self.matrix = self.matrix + torch.matmul(w.unsqueeze(-1), a.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head 모듈 구현\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, memory: Memory, hidden_size: int) -> None:\n",
    "        super(Head, self).__init__()\n",
    "        self.memory = memory\n",
    "        self.k_layer = nn.Linear(hidden_size, self.memory.size[1]) # [1, M], key 벡터 생성\n",
    "        self.beta_layer = nn.Linear(hidden_size, 1)\n",
    "        self.g_layer = nn.Linear(hidden_size, 1)\n",
    "        self.s_layer = nn.Linear(hidden_size, 3)\n",
    "        self.gamma_layer = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        for layer in [self.k_layer, self.beta_layer, self.g_layer, self.s_layer, self.gamma_layer]:\n",
    "            nn.init.xavier_uniform_(layer.weight, gain=1.4)\n",
    "            nn.init.normal_(layer.bias, std=0.01)\n",
    "        self._initial_state = nn.Parameter(torch.randn(1, self.memory.size[0]) * 1e-5) # [1, N]\n",
    "    \n",
    "    def get_initial_state(self, batch_size: int) -> torch.Tensor:\n",
    "        return F.softmax(self._initial_state, dim=1).repeat(batch_size, 1) # [batch_size, N]\n",
    "    \n",
    "    def get_head_weight(\n",
    "        self, x: torch.Tensor, previous_state: torch.Tensor, memory_matrix: torch.Tensor\n",
    "    ) -> torch.Tensor: # previous_stae는 과거 가중치임\n",
    "        k = self.k_layer(x)\n",
    "        beta = F.softplus(self.beta_layer(x))\n",
    "        g = torch.sigmoid(self.g_layer(x))\n",
    "        s = F.softmax(self.s_layer(x), dim=1) # [batch, 3]\n",
    "        gamma = 1 + F.softplus(self.gamma_layer(x))\n",
    "\n",
    "        w_c = F.softmax(\n",
    "            beta * F.cosine_similarity(memory_matrix + 1e-16, k.unsqueeze(1) + 1e-16, dim=-1), dim=1\n",
    "        ) # [batch_size, N]\n",
    "        #print(previous_state.shape)\n",
    "        #print(w_c.shape)\n",
    "        w_g = g * w_c + (1-g) * previous_state \n",
    "        w_t = self._shift(w_g, s)\n",
    "        \n",
    "        w = w_t**gamma\n",
    "        w = torch.div(w, torch.sum(w, dim=1).unsqueeze(1) + 1e-16)\n",
    "        return w # [batch_size, N]\n",
    "    \n",
    "    def _convolve(self, w: torch.Tensor, s: torch.Tensor) -> torch.Tensor:\n",
    "        assert s.size(0) == 3 # s의 size(0)은 3이 되어야 하는 것을 보장함. 여기서 들어오는 s는 batch 중 하나의 s임. 크기 [3], w도 마찬가지임. 크기 [N]\n",
    "        t = torch.cat([w[-1:], w, w[:1]], dim=0) # convolution을 위한 일종의 padding 작업\n",
    "        c = F.conv1d(t.view(1, 1, -1), s.view(1, 1, -1)).view(-1)\n",
    "        return c # [N]\n",
    "    \n",
    "    def _shift(self, w_g: torch.Tensor, s: torch.Tensor) -> torch.Tensor:\n",
    "        result = w_g.clone()\n",
    "        for b in range(len(w_g)):\n",
    "            result[b] = self._convolve(w_g[b], s[b])\n",
    "        return result # [batch_size, N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReadHead, WriteHead 모듈 구현\n",
    "class ReadHead(Head):\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, previous_state: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "       w = self.get_head_weight(x, previous_state, self.memory.matrix)\n",
    "\n",
    "       return torch.matmul(w.unsqueeze(1), self.memory.matrix).squeeze(1), w  # read vector(=[batch_size, M]와 계산된 w 반환(previous state로 활용)\n",
    "\n",
    "class WriteHead(Head):\n",
    "    def __init__(self, memory: Memory, hidden_size: int) -> None:\n",
    "        super(WriteHead, self).__init__(memory=memory, hidden_size=hidden_size)\n",
    "        self.e_layer = nn.Linear(hidden_size, memory.size[1]) # 삭제벡터 생성을 위한 layer, [batch_size, M]이여야 함. \n",
    "        self.a_layer = nn.Linear(hidden_size, memory.size[1]) # 추가벡터 생성을 위하 layer, [batch_size, M]이여야 함.\n",
    "\n",
    "        # 가중치 초기화\n",
    "        for layer in [self.e_layer, self.a_layer]:\n",
    "            nn.init.xavier_uniform_(layer.weight, gain=1.4)\n",
    "            nn.init.normal_(layer.bias, std=0.01)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, previous_state: torch.Tensor) -> torch.Tensor:\n",
    "        w = self.get_head_weight(x, previous_state, self.memory.matrix)\n",
    "        e = torch.sigmoid(self.e_layer(x))\n",
    "        a = self.a_layer(x)\n",
    "\n",
    "        self.memory.write(w, e, a)\n",
    "        return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "'''  디버깅\n",
    "batch_size = 32\n",
    "num_ways = 5\n",
    "memory = Memory(size=(128, 40))\n",
    "memory.reset(batch_size=batch_size)\n",
    "wh = WriteHead(memory, hidden_size=200)\n",
    "input = torch.randn(batch_size, 200)\n",
    "write_head_state = wh.get_initial_state(batch_size)\n",
    "w = wh.get_head_weight(input, write_head_state, memory_matrix=memory.matrix)\n",
    "e = torch.sigmoid(wh.e_layer(input))\n",
    "a = wh.a_layer(input)\n",
    "memory.write(w, e, a)\n",
    "output = wh(input, write_head_state) \n",
    "print(output.shape) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controller 모듈 구현\n",
    "class LSTMController(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int) -> None:\n",
    "        super(LSTMController, self).__init__()\n",
    "        self.layer = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers = 1, batch_first=False) # batch_first=False면 input을 [seq_len, batch_size, input_size]로 받아야 함. \n",
    "        self.lstm_h_state = nn.Parameter(torch.randn(1, 1, hidden_size) * 0.05) # [num_layers, 1, hidden_size]\n",
    "        self.lstm_c_state = nn.Parameter(torch.randn(1, 1, hidden_size) * 0.05) # [num_layers, 1, hidden_size]\n",
    "\n",
    "        # 가중치 초기화\n",
    "        for p in self.layer.parameters():\n",
    "            if p.dim() == 1:\n",
    "                nn.init.constant_(p, 0)\n",
    "            else:\n",
    "                stdev = 5 / (np.sqrt(input_size + hidden_size))\n",
    "                nn.init.uniform_(p, -stdev, stdev)\n",
    "    \n",
    "    def get_initial_state(self, batch_size: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        lstm_h = self.lstm_h_state.clone().repeat(1, batch_size, 1) # [num_layers, batch_size, hidden_size], LSTM은 이렇게 받아야 함. \n",
    "        lstm_c = self.lstm_c_state.clone().repeat(1, batch_size, 1) # [num_layers, batch_size, hidden_size]\n",
    "        return lstm_h, lstm_c\n",
    "    \n",
    "    def forward(\n",
    "        self, x: torch.Tensor, state: Tuple[torch.Tensor]\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor]]:\n",
    "        output, state = self.layer(x.unsqueeze(0), state)\n",
    "        return output.squeeze(0), state # output -> [batch_size, hidden_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANN 구현\n",
    "class MANN(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_ways: int, img_size: int = 28, memory_size: Tuple[int, int] = (128, 40), hidden_size: int = 200,\n",
    "    ) -> None:\n",
    "       super(MANN, self).__init__()\n",
    "       input_size = img_size * img_size + num_ways\n",
    "       controller_input_size = input_size + memory_size[1]\n",
    "\n",
    "       self.memory = Memory(size=memory_size)\n",
    "       self.read_head = ReadHead(memory=self.memory, hidden_size=hidden_size)\n",
    "       self.write_head = WriteHead(memory=self.memory, hidden_size=hidden_size)\n",
    "       self.controller = LSTMController(input_size=controller_input_size, hidden_size=hidden_size)\n",
    "\n",
    "       self.fc = nn.Linear(hidden_size + memory_size[1], num_ways)\n",
    "       nn.init.xavier_uniform_(self.fc.weight, gain=1)\n",
    "       nn.init.normal_(self.fc.bias, std=0.01)\n",
    "    \n",
    "    def get_initial_state(\n",
    "        self, batch_size: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, Tuple[torch.Tensor]]:\n",
    "        self.memory.reset(batch_size)\n",
    "        read = self.memory.get_initial_read(batch_size)\n",
    "        read_head_state = self.read_head.get_initial_state(batch_size)\n",
    "        write_head_state = self.write_head.get_initial_state(batch_size)\n",
    "        controller_state = self.controller.get_initial_state(batch_size)\n",
    "        return (read, read_head_state, write_head_state, controller_state)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x:torch.Tensor,\n",
    "        previous_state: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Tuple[torch.Tensor]],\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Tuple[torch.Tensor]]]:\n",
    "       (previous_read, previous_read_head_state, previous_write_head_state, previous_controller_state) = previous_state\n",
    "       \n",
    "       controller_input = torch.cat([x, previous_read], dim=1)\n",
    "       controller_output, controller_state = self.controller(controller_input, previous_controller_state)\n",
    "       read_head_output, read_head_state = self.read_head(controller_output, previous_read_head_state)\n",
    "       write_head_state = self.write_head(controller_output, previous_write_head_state)\n",
    "\n",
    "       fc_input = torch.cat((controller_output, read_head_output), dim=1)\n",
    "       state = (read_head_output, read_head_state, write_head_state, controller_state)\n",
    "       return F.softmax(self.fc(fc_input), dim=1), state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메타 데이터셋 변환 함수 정의\n",
    "def generate_sequence(xs: torch.Tensor, ys: torch.Tensor, num_ways: int, device:str) -> torch.Tensor:\n",
    "    xs_flat = xs.flatten(2, 4) # [batch_size, num_ways * num_shots, C*H*W]\n",
    "\n",
    "    ys_onehot = F.one_hot(ys, num_classes=num_ways)\n",
    "    ys_cat = torch.cat(\n",
    "        (torch.zeros(ys_onehot.shape[0], 1, ys_onehot.shape[2]).to(device), ys_onehot), dim=1\n",
    "    )[:, :-1, :] # [batch_size, num_ways * num_shots, num_ways], 한칸씩 label이 밀림. \n",
    "\n",
    "    seq = torch.cat((xs_flat, ys_cat), dim=2) # [batch_size, num_ways * num_shots, C*H*W + num_ways]\n",
    "    return torch.swapaxes(seq, 0, 1) # [num_ways * num_shots, batch_size, C*H*W + num_ways]\n",
    "\n",
    "def generate_sequence_v2(\n",
    "    task_batch: Dict[str, List[torch.Tensor]], device: str, num_ways: int\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    support_xs = task_batch[\"train\"][0].to(device=device) # [batch_size, num_shots*num_ways, C, H, W]\n",
    "    support_ys = task_batch[\"train\"][1].to(device=device) # [batch_size, num_shots*num_ways]\n",
    "    query_xs = task_batch[\"test\"][0].to(device=device) # [batch_size, num_shots*num_ways, C, H, W]\n",
    "    query_ys = task_batch[\"test\"][1].to(device=device) # [batch_size, num_shots*num_ways]\n",
    "\n",
    "    random_indices = torch.randperm(5)\n",
    "    query_xs_ = query_xs[:, random_indices, :, :, :]\n",
    "    query_ys_ = query_ys[:, random_indices]\n",
    "\n",
    "    support_seq = generate_sequence(xs=support_xs, ys=support_ys, num_ways=num_ways, device=device)\n",
    "    query_seq = generate_sequence(xs=query_xs_, ys=query_ys_, num_ways=num_ways, device=device)\n",
    "\n",
    "    x_seq = torch.cat((support_seq, query_seq), dim=0)\n",
    "    y_seq = torch.cat((support_ys, query_ys_), dim=1)\n",
    "    return x_seq, y_seq # x_seq -> [num_ways * num_shots * 2, batch_size, C*H*W + num_ways] y_seq -> [batch_size , num_shots*num_ways*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수 정의 \n",
    "def train_mann(\n",
    "    num_ways: int,\n",
    "    num_shots: int,\n",
    "    task_batch_size: int,\n",
    "    device: str, \n",
    "    task_batch: Dict[str, List[torch.Tensor]],\n",
    "    model: MANN,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    "    optimizer: torch.optim.RMSprop,\n",
    ") -> Tuple[float, float]:\n",
    "   \n",
    "   model.train()\n",
    "   optimizer.zero_grad()\n",
    "\n",
    "   x_seq_, y_seq_ = generate_sequence_v2(task_batch=task_batch, device=device, num_ways=num_ways)\n",
    "   x_seq = x_seq_.to(device=device)\n",
    "   y_seq = y_seq_.to(device=device)\n",
    "\n",
    "   state = model.get_initial_state(batch_size=task_batch_size)\n",
    "   prob = torch.zeros((len(x_seq), task_batch_size, num_ways)).to(device=device) # [num_ways * num_shots * 2, batch_size, num_ways]\n",
    "   # num_timestep = num_ways * num_shots\n",
    "   for j, vector in enumerate(x_seq):\n",
    "      prob[j], state = model(vector, state)\n",
    "   \n",
    "   prob_ = prob.permute(1, 2, 0) # [batch_size, num_ways, num_ways * num_shots * 2]\n",
    "   loss = criterion(prob_, y_seq)\n",
    "    \n",
    "   loss.backward()\n",
    "   optimizer.step()\n",
    "   \n",
    "   with torch.no_grad():\n",
    "      num_query = num_ways * num_shots # 정확도 계산시 query set만 이용\n",
    "      correct = torch.sum(prob_[..., -num_query:].argmax(dim=1) == y_seq[..., -num_query:])\n",
    "      accuracy = correct.item() / np.prod(y_seq[..., -num_query:].size())\n",
    "   return accuracy, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 함수 정의\n",
    "def test_mann(\n",
    "    num_ways: int,\n",
    "    num_shots: int,\n",
    "    task_batch_size: int,\n",
    "    device: str, \n",
    "    task_batch: Dict[str, List[torch.Tensor]],\n",
    "    model: MANN,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    ") -> Tuple[float, float]:\n",
    "   \n",
    "   model.eval()\n",
    "\n",
    "   x_seq_, y_seq_ = generate_sequence_v2(task_batch=task_batch, device=device, num_ways=num_ways)\n",
    "   x_seq = x_seq_.to(device=device)\n",
    "   y_seq = y_seq_.to(device=device)\n",
    "\n",
    "   state = model.get_initial_state(batch_size=task_batch_size)\n",
    "   prob = torch.zeros((len(x_seq), task_batch_size, num_ways)).to(device=device) # [num_ways * num_shots * 2, batch_size, num_ways]\n",
    "   # num_timestep = num_ways * num_shots\n",
    "   for j, vector in enumerate(x_seq):\n",
    "      prob[j], state = model(vector, state)\n",
    "   \n",
    "   prob_ = prob.permute(1, 2, 0) # [batch_size, num_ways, num_ways * num_shots * 2]\n",
    "   loss = criterion(prob_, y_seq)\n",
    "    \n",
    "   \n",
    "   with torch.no_grad():\n",
    "      num_query = num_ways * num_shots # 정확도 계산시 query set만 이용\n",
    "      correct = torch.sum(prob_[..., -num_query:].argmax(dim=1) == y_seq[..., -num_query:])\n",
    "      accuracy = correct.item() / np.prod(y_seq[..., -num_query:].size())\n",
    "   return accuracy, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(\n",
    "    config: Dict[str, Any]\n",
    ") -> Tuple[BatchMetaDataLoader, BatchMetaDataLoader, BatchMetaDataLoader]:\n",
    "    train_dataset = omniglot(\n",
    "        folder=config[\"folder_name\"],\n",
    "        shots=config[\"num_shots\"],\n",
    "        # test_shots=1, # default = shots\n",
    "        ways=config[\"num_ways\"],\n",
    "        shuffle=True,\n",
    "        meta_train=True,\n",
    "        download=config[\"download\"],\n",
    "    )\n",
    "    train_dataloader = BatchMetaDataLoader(\n",
    "        train_dataset, batch_size=config[\"task_batch_size\"], shuffle=True, num_workers=0\n",
    "    )\n",
    "\n",
    "    val_dataset = omniglot(\n",
    "        folder=config[\"folder_name\"],\n",
    "        shots=config[\"num_shots\"],\n",
    "        # test_shots=1, # default = shots\n",
    "        ways=config[\"num_ways\"],\n",
    "        shuffle=True,\n",
    "        meta_val=True,\n",
    "        download=config[\"download\"],\n",
    "    )\n",
    "    val_dataloader = BatchMetaDataLoader(\n",
    "        val_dataset, batch_size=config[\"task_batch_size\"], shuffle=True, num_workers=0\n",
    "    )\n",
    "\n",
    "    test_dataset = omniglot(\n",
    "        folder=config[\"folder_name\"],\n",
    "        shots=config[\"num_shots\"],\n",
    "        # test_shots=1, # default = shots\n",
    "        ways=config[\"num_ways\"],\n",
    "        shuffle=True,\n",
    "        meta_test=True,\n",
    "        download=config[\"download\"],\n",
    "    )\n",
    "    test_dataloader = BatchMetaDataLoader(\n",
    "        test_dataset, batch_size=config[\"task_batch_size\"], shuffle=True, num_workers=0\n",
    "    )\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph 그리는 함수\n",
    "def print_graph(\n",
    "    train_accuracies: List[float],\n",
    "    val_accuracies: List[float],\n",
    "    train_losses: List[float],\n",
    "    val_losses: List[float],\n",
    ") -> None:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    axs[0].plot(train_accuracies, label=\"train_acc\")\n",
    "    axs[0].plot(val_accuracies, label=\"test_acc\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(train_losses, label=\"train_loss\")\n",
    "    axs[1].plot(val_losses, label=\"test_loss\")\n",
    "    axs[1].set_title(\"Loss\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(output_folder: str, model: MANN, title: str) -> None:\n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "    filename = os.path.join(output_folder, title)\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        state_dict = model.state_dict()\n",
    "        torch.save(state_dict, f)\n",
    "    print(\"Model is saved in\", filename)\n",
    "\n",
    "\n",
    "def load_model(output_folder: str, model: MANN, title: str) -> None:\n",
    "    filename = os.path.join(output_folder, title)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    print(\"Model is loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"folder_name\": \"../load_dataset/dataset\",\n",
    "    \"download\": False,\n",
    "    \"num_shots\": 1,\n",
    "    \"num_ways\": 5,\n",
    "    \"output_folder\": \"saved_model\",\n",
    "    \"task_batch_size\": 32,  # 필수\n",
    "    \"num_task_batch_train\": 100000,  # 필수\n",
    "    \"num_task_batch_test\": 30000,  # 필수\n",
    "    \"device\": \"cuda\",  # 필수\n",
    "}\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = get_dataloader(config)\n",
    "\n",
    "model = MANN(num_ways=config[\"num_ways\"]).to(device=config[\"device\"])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), momentum=0.9, alpha=0.95, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메타-트레이닝 (에폭은 한번만)\n",
    "with tqdm(\n",
    "    zip(train_dataloader, val_dataloader), total=config[\"num_task_batch_train\"]\n",
    ") as pbar:\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for task_batch_idx, (train_batch, val_batch) in enumerate(pbar):\n",
    "        if task_batch_idx >= config[\"num_task_batch_train\"]:\n",
    "            break\n",
    "\n",
    "        train_accuracy, train_loss = train_mann(\n",
    "            num_ways=config[\"num_ways\"],\n",
    "            num_shots=config[\"num_shots\"],\n",
    "            task_batch_size=config[\"task_batch_size\"],\n",
    "            device=config[\"device\"],\n",
    "            task_batch=train_batch,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "        )\n",
    "        val_accuracy, val_loss = test_mann(\n",
    "            num_ways=config[\"num_ways\"],\n",
    "            num_shots=config[\"num_shots\"],\n",
    "            task_batch_size=config[\"task_batch_size\"],\n",
    "            device=config[\"device\"],\n",
    "            task_batch=val_batch,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "        )\n",
    "\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            train_accuracy=\"{0:.4f}\".format(train_accuracy),\n",
    "            val_accuracy=\"{0:.4f}\".format(val_accuracy),\n",
    "            train_loss=\"{0:.4f}\".format(train_loss),\n",
    "            val_loss=\"{0:.4f}\".format(val_loss),\n",
    "        )\n",
    "\n",
    "    # 모델 저장하기\n",
    "    save_model(output_folder=config[\"output_folder\"], model=model, title=\"mann.th\")\n",
    "\n",
    "    print_graph(\n",
    "        train_accuracies=train_accuracies,\n",
    "        val_accuracies=val_accuracies,\n",
    "        train_losses=train_losses,\n",
    "        val_losses=val_losses,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "load_model(output_folder=config[\"output_folder\"], model=model, title=\"mann.th\")\n",
    "\n",
    "# 메타-테스팅\n",
    "with tqdm(test_dataloader, total=config[\"num_task_batch_test\"]) as pbar:\n",
    "    sum_test_accuracies = 0.0\n",
    "    sum_test_losses = 0.0\n",
    "\n",
    "    for task_batch_idx, test_batch in enumerate(pbar):\n",
    "        if task_batch_idx >= config[\"num_task_batch_test\"]:\n",
    "            break\n",
    "\n",
    "        test_accuracy, test_loss = test_mann(\n",
    "            num_ways=config[\"num_ways\"],\n",
    "            num_shots=config[\"num_shots\"],\n",
    "            task_batch_size=config[\"task_batch_size\"],\n",
    "            device=config[\"device\"],\n",
    "            task_batch=test_batch,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "        )\n",
    "\n",
    "        sum_test_accuracies += test_accuracy\n",
    "        sum_test_losses += test_loss\n",
    "        pbar.set_postfix(\n",
    "            test_accuracy=\"{0:.4f}\".format(sum_test_accuracies / (task_batch_idx + 1)),\n",
    "            test_loss=\"{0:.4f}\".format(sum_test_losses / (task_batch_idx + 1)),\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "543a2858b738eacd0ac3a1925ec6f5fb4c9073862b17acbfaa542a46096ac3bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
