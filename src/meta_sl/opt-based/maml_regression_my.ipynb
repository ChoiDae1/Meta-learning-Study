{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmeta.modules import MetaLinear, MetaModule, MetaSequential\n",
    "from torchmeta.toy import Sinusoid\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "from torchmeta.utils.gradient_based import gradient_update_parameters\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression model 정의\n",
    "class FCNet(MetaModule):\n",
    "    def __init__(self, in_dimension: int, out_dimension: int) -> None:\n",
    "        super(FCNet, self).__init__()\n",
    "        self.in_dimension = in_dimension\n",
    "        self.out_dimension = out_dimension\n",
    "        self.hidden_size = 40\n",
    "        \n",
    "        self.linears = MetaSequential(\n",
    "            MetaLinear(self.in_dimension, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            MetaLinear(self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            MetaLinear(self.hidden_size, self.out_dimension),\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self, x: torch.Tensor, params: Optional[collections.OrderedDict] = None\n",
    "    ) -> torch.Tensor:\n",
    "        pred = self.linears(x, params=self.get_subdict(params, \"linears\"))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습함수 정의 -> 이게 핵심임. \n",
    "def train_maml(\n",
    "    num_shots: int,\n",
    "    device: str,\n",
    "    task_batch_size: int,\n",
    "    task_batch: Tuple[torch.Tensor, torch.Tensor],\n",
    "    model: FCNet, \n",
    "    criterion: nn.MSELoss, # regression이므로\n",
    "    optimizer: torch.optim.Adam,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    xs, ys = task_batch\n",
    "    support_xs = xs[:, : num_shots, :].to(device=device).type(torch.float) # [batch_size, num_shots, 1]\n",
    "    query_xs = xs[:, num_shots:, :].to(device=device).type(torch.float) # [batch_size, num_shots, 1]\n",
    "    support_ys = ys[:, : num_shots, :].to(device=device).type(torch.float) # [batch_size, num_shots, 1]\n",
    "    query_ys= ys[:, num_shots:, :].to(device=device).type(torch.float) # [batch_size, num_shots, 1]\n",
    "\n",
    "    outer_loss = torch.tensor(0.0, device=device)\n",
    "    # inner loop 진행 + task마다 outer loop를 계산을 위해 계산\n",
    "    for support_x, support_y, query_x, query_y in zip(support_xs, support_ys, query_xs, query_ys): # support_x : [num_shots, 1]\n",
    "        support_pred = model(support_x)\n",
    "        inner_loss = criterion(support_pred, support_y)\n",
    "\n",
    "        params = gradient_update_parameters(model, inner_loss, step_size=0.01, first_order=False)\n",
    "\n",
    "        query_pred = model(query_x, params=params)\n",
    "        outer_loss += criterion(query_pred, query_y)\n",
    "    \n",
    "    outer_loss.div_(task_batch_size)\n",
    "\n",
    "    outer_loss.backward()\n",
    "    optimizer.step()\n",
    "    return outer_loss.item()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 함수 정의 \n",
    "def test_maml(\n",
    "    num_shots: int,\n",
    "    device: str,\n",
    "    task_batch_size: int,\n",
    "    task_batch: Tuple[torch.Tensor, torch.Tensor],\n",
    "    model: FCNet, \n",
    "    criterion: nn.MSELoss, # regression이므로\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    \n",
    "    xs, ys = task_batch\n",
    "    support_xs = xs[:, : num_shots, :].to(device=device).type(torch.float) # [batch_size, num_shots, 1]\n",
    "    query_xs = xs[:, num_shots:, :].to(device=device).type(torch.float) # [batch_size, num_shots, 1]\n",
    "    support_ys = ys[:, : num_shots, :].to(device=device).type(torch.float) # [batch_size, num_shots, 1]\n",
    "    query_ys= ys[:, num_shots:, :].to(device=device).type(torch.float) # [batch_size, num_shots, 1]\n",
    "\n",
    "    outer_loss = torch.tensor(0.0, device=device)\n",
    "    # inner loop 진행 + task마다 outer loop를 계산을 위해 계산\n",
    "    for support_x, support_y, query_x, query_y in zip(support_xs, support_ys, query_xs, query_ys): # support_x : [num_shots, 1]\n",
    "        support_pred = model(support_x)\n",
    "        inner_loss = criterion(support_pred, support_y)\n",
    "\n",
    "        params = gradient_update_parameters(model, inner_loss, step_size=0.01, first_order=False)\n",
    "\n",
    "        query_pred = model(query_x, params=params)\n",
    "        outer_loss += criterion(query_pred, query_y)\n",
    "    \n",
    "    outer_loss.div_(task_batch_size)\n",
    "\n",
    "    return outer_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(\n",
    "    config: Dict[str, int]\n",
    ") -> Tuple[BatchMetaDataLoader, BatchMetaDataLoader, BatchMetaDataLoader]:\n",
    "    train_dataset = Sinusoid(\n",
    "        num_samples_per_task=config[\"num_shots\"] * 2,\n",
    "        num_tasks=config[\"num_task_batch_train\"] * config[\"task_batch_size\"],\n",
    "        noise_std=None,\n",
    "    )\n",
    "    train_dataloader = BatchMetaDataLoader(\n",
    "        train_dataset, batch_size=config[\"task_batch_size\"]\n",
    "    )\n",
    "\n",
    "    val_dataset = Sinusoid(\n",
    "        num_samples_per_task=config[\"num_shots\"] * 2,\n",
    "        num_tasks=config[\"num_task_batch_train\"] * config[\"task_batch_size\"],\n",
    "        noise_std=None,\n",
    "    )\n",
    "    val_dataloader = BatchMetaDataLoader(\n",
    "        val_dataset, batch_size=config[\"task_batch_size\"]\n",
    "    )\n",
    "\n",
    "    test_dataset = Sinusoid(\n",
    "        num_samples_per_task=config[\"num_shots\"] * 2,\n",
    "        num_tasks=config[\"num_task_batch_test\"] * config[\"task_batch_size\"],\n",
    "        noise_std=None,\n",
    "    )\n",
    "    test_dataloader = BatchMetaDataLoader(\n",
    "        test_dataset, batch_size=config[\"task_batch_size\"]\n",
    "    )\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_shots\": 10,\n",
    "    \"output_folder\": \"saved_model\",\n",
    "    \"task_batch_size\": 100,  # 필수\n",
    "    \"num_task_batch_train\": 1000,  # 필수\n",
    "    \"num_task_batch_test\": 300,  # 필수\n",
    "    \"device\": \"cuda\",  # 필수\n",
    "}\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = get_dataloader(config)\n",
    "\n",
    "model = FCNet(in_dimension=1, out_dimension=1).to(device=config[\"device\"])\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(output_folder: str, model: FCNet, title: str) -> None:\n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "    filename = os.path.join(output_folder, title)\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        state_dict = model.state_dict()\n",
    "        torch.save(state_dict, f)\n",
    "    print(\"Model is saved in\", filename)\n",
    "\n",
    "\n",
    "def load_model(output_folder: str, model: FCNet, title: str) -> None:\n",
    "    filename = os.path.join(output_folder, title)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    print(\"Model is loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_graph(train_losses: List[float], val_losses: List[float]) -> None:\n",
    "    plt.figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "    plt.plot(train_losses, label=\"train_loss\")\n",
    "    plt.plot(val_losses, label=\"val_loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [16:10<00:00,  1.03it/s, train_loss=0.8710, val_loss=0.8665]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# 메타-트레이닝\n",
    "with tqdm(\n",
    "    zip(train_dataloader, val_dataloader), total=config[\"num_task_batch_train\"]\n",
    ") as pbar:\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for task_batch_idx, (train_batch, val_batch) in enumerate(pbar):\n",
    "        train_loss = train_maml(\n",
    "            num_shots=config[\"num_shots\"],\n",
    "            device=config[\"device\"],\n",
    "            task_batch_size=config[\"task_batch_size\"],\n",
    "            task_batch=train_batch,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "        )\n",
    "        val_loss = test_maml(\n",
    "            num_shots=config[\"num_shots\"],\n",
    "            device=config[\"device\"],\n",
    "            task_batch_size=config[\"task_batch_size\"],\n",
    "            task_batch=train_batch,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "        )\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            train_loss=\"{0:.4f}\".format(train_loss), val_loss=\"{0:.4f}\".format(val_loss)\n",
    "        )\n",
    "\n",
    "'''# 모델 저장하기\n",
    "save_model(\n",
    "    output_folder=config[\"output_folder\"], model=model, title=\"maml_regression.th\"\n",
    ")'''\n",
    "\n",
    "print_loss_graph(train_losses=train_losses, val_losses=val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "load_model(\n",
    "    output_folder=config[\"output_folder\"], model=model, title=\"maml_regression.th\"\n",
    ")\n",
    "\n",
    "# 메타-테스팅\n",
    "with tqdm(test_dataloader, total=config[\"num_task_batch_test\"]) as pbar:\n",
    "    sum_test_losses = 0.0\n",
    "\n",
    "    for task_batch_idx, test_batch in enumerate(pbar):\n",
    "        test_loss = test_maml(\n",
    "            num_shots=config[\"num_shots\"],\n",
    "            device=config[\"device\"],\n",
    "            task_batch_size=config[\"task_batch_size\"],\n",
    "            task_batch=train_batch,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "        )\n",
    "\n",
    "        sum_test_losses += test_loss\n",
    "        pbar.set_postfix(\n",
    "            test_loss=\"{0:.4f}\".format(sum_test_losses / (task_batch_idx + 1))\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "543a2858b738eacd0ac3a1925ec6f5fb4c9073862b17acbfaa542a46096ac3bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
