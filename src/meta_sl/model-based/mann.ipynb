{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T06:16:38.965227Z",
     "iopub.status.busy": "2022-07-03T06:16:38.964926Z",
     "iopub.status.idle": "2022-07-03T06:16:40.320710Z",
     "shell.execute_reply": "2022-07-03T06:16:40.319682Z"
    },
    "id": "Ker6IKfjdvCD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmeta.datasets.helpers import omniglot\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T06:16:40.325194Z",
     "iopub.status.busy": "2022-07-03T06:16:40.324923Z",
     "iopub.status.idle": "2022-07-03T06:16:40.333804Z",
     "shell.execute_reply": "2022-07-03T06:16:40.332878Z"
    },
    "id": "bSx9lX2wdwQg"
   },
   "outputs": [],
   "source": [
    "def get_dataloader(\n",
    "    config: Dict[str, Any]\n",
    ") -> Tuple[BatchMetaDataLoader, BatchMetaDataLoader, BatchMetaDataLoader]:\n",
    "    train_dataset = omniglot(\n",
    "        folder=config[\"folder_name\"],\n",
    "        shots=config[\"num_shots\"],\n",
    "        # test_shots=1, # default = shots\n",
    "        ways=config[\"num_ways\"],\n",
    "        shuffle=True,\n",
    "        meta_train=True,\n",
    "        download=config[\"download\"],\n",
    "    )\n",
    "    train_dataloader = BatchMetaDataLoader(\n",
    "        train_dataset, batch_size=config[\"task_batch_size\"], shuffle=True, num_workers=0\n",
    "    )\n",
    "\n",
    "    val_dataset = omniglot(\n",
    "        folder=config[\"folder_name\"],\n",
    "        shots=config[\"num_shots\"],\n",
    "        # test_shots=1, # default = shots\n",
    "        ways=config[\"num_ways\"],\n",
    "        shuffle=True,\n",
    "        meta_val=True,\n",
    "        download=config[\"download\"],\n",
    "    )\n",
    "    val_dataloader = BatchMetaDataLoader(\n",
    "        val_dataset, batch_size=config[\"task_batch_size\"], shuffle=True, num_workers=0\n",
    "    )\n",
    "\n",
    "    test_dataset = omniglot(\n",
    "        folder=config[\"folder_name\"],\n",
    "        shots=config[\"num_shots\"],\n",
    "        # test_shots=1, # default = shots\n",
    "        ways=config[\"num_ways\"],\n",
    "        shuffle=True,\n",
    "        meta_test=True,\n",
    "        download=config[\"download\"],\n",
    "    )\n",
    "    test_dataloader = BatchMetaDataLoader(\n",
    "        test_dataset, batch_size=config[\"task_batch_size\"], shuffle=True, num_workers=0\n",
    "    )\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T06:16:40.338272Z",
     "iopub.status.busy": "2022-07-03T06:16:40.337685Z",
     "iopub.status.idle": "2022-07-03T06:16:40.346586Z",
     "shell.execute_reply": "2022-07-03T06:16:40.345631Z"
    },
    "id": "kF2l3CBOd089"
   },
   "outputs": [],
   "source": [
    "class Memory(nn.Module):\n",
    "    def __init__(self, size: int) -> None:\n",
    "        super(Memory, self).__init__()\n",
    "        self.size = size\n",
    "\n",
    "        initial_state = torch.ones(self.size) * 1e-6\n",
    "        self.register_buffer(\"initial_state\", initial_state.data)\n",
    "\n",
    "        self.initial_read = nn.Parameter(torch.randn(1, self.size[1]) * 0.01)\n",
    "\n",
    "    def reset(self, batch_size: int) -> None:\n",
    "        self.matrix = self.initial_state.clone().repeat(batch_size, 1, 1)\n",
    "\n",
    "    def get_initial_read(self, batch_size: int) -> torch.Tensor:\n",
    "        return self.initial_read.clone().repeat(batch_size, 1)\n",
    "\n",
    "    def write(self, w: torch.Tensor, e: torch.Tensor, a: torch.Tensor) -> None:\n",
    "        self.matrix = self.matrix * (1 - torch.matmul(w.unsqueeze(-1), e.unsqueeze(1)))\n",
    "        self.matrix = self.matrix + torch.matmul(w.unsqueeze(-1), a.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T06:16:40.350334Z",
     "iopub.status.busy": "2022-07-03T06:16:40.349859Z",
     "iopub.status.idle": "2022-07-03T06:16:40.365880Z",
     "shell.execute_reply": "2022-07-03T06:16:40.365053Z"
    },
    "id": "gKeQJajHeSCP"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, memory: Memory, hidden_size: int) -> None:\n",
    "        super(Head, self).__init__()\n",
    "        self.memory = memory\n",
    "        self.k_layer = nn.Linear(hidden_size, self.memory.size[1])\n",
    "        self.beta_layer = nn.Linear(hidden_size, 1)\n",
    "        self.g_layer = nn.Linear(hidden_size, 1)\n",
    "        self.s_layer = nn.Linear(hidden_size, 3)\n",
    "        self.gamma_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        for layer in [\n",
    "            self.k_layer,\n",
    "            self.beta_layer,\n",
    "            self.g_layer,\n",
    "            self.s_layer,\n",
    "            self.gamma_layer,\n",
    "        ]:\n",
    "            nn.init.xavier_uniform_(layer.weight, gain=1.4)\n",
    "            nn.init.normal_(layer.bias, std=0.01)\n",
    "\n",
    "        self._initial_state = nn.Parameter(torch.randn(1, self.memory.size[0]) * 1e-5)\n",
    "\n",
    "    def get_initial_state(self, batch_size: int) -> torch.Tensor:\n",
    "        return F.softmax(self._initial_state, dim=1).repeat(batch_size, 1)\n",
    "\n",
    "    def get_head_weight(\n",
    "        self, x: torch.Tensor, previous_state: torch.Tensor, memory_matrix: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        k = self.k_layer(x)\n",
    "        beta = F.softplus(self.beta_layer(x))\n",
    "        g = torch.sigmoid(self.g_layer(x))\n",
    "        s = F.softmax(self.s_layer(x), dim=1)\n",
    "        gamma = 1 + F.softplus(self.gamma_layer(x))\n",
    "\n",
    "        w_c = F.softmax(\n",
    "            beta\n",
    "            * F.cosine_similarity(\n",
    "                memory_matrix + 1e-16, k.unsqueeze(1) + 1e-16, dim=-1\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "        w_g = g * w_c + (1 - g) * previous_state\n",
    "        w_t = self._shift(w_g, s)\n",
    "\n",
    "        w = w_t**gamma\n",
    "        w = torch.div(w, torch.sum(w, dim=1).unsqueeze(1) + 1e-16)\n",
    "        return w\n",
    "\n",
    "    def _convolve(self, w: torch.Tensor, s: torch.Tensor) -> torch.Tensor:\n",
    "        assert s.size(0) == 3\n",
    "        t = torch.cat([w[-1:], w, w[:1]], dim=0)\n",
    "        c = F.conv1d(t.view(1, 1, -1), s.view(1, 1, -1)).view(-1)\n",
    "        return c\n",
    "\n",
    "    def _shift(self, w_g: torch.Tensor, s: torch.Tensor) -> torch.Tensor:\n",
    "        result = w_g.clone()\n",
    "        for b in range(len(w_g)):\n",
    "            result[b] = self._convolve(w_g[b], s[b])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T06:16:40.369495Z",
     "iopub.status.busy": "2022-07-03T06:16:40.369331Z",
     "iopub.status.idle": "2022-07-03T06:16:40.373866Z",
     "shell.execute_reply": "2022-07-03T06:16:40.373111Z"
    },
    "id": "CbgTDvIweXg-"
   },
   "outputs": [],
   "source": [
    "class ReadHead(Head):\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, previous_state: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        w = self.get_head_weight(x, previous_state, self.memory.matrix)\n",
    "        return torch.matmul(w.unsqueeze(1), self.memory.matrix).squeeze(1), w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T06:16:40.376877Z",
     "iopub.status.busy": "2022-07-03T06:16:40.376637Z",
     "iopub.status.idle": "2022-07-03T06:16:40.384367Z",
     "shell.execute_reply": "2022-07-03T06:16:40.383552Z"
    },
    "id": "zzo3ISvVeZNG"
   },
   "outputs": [],
   "source": [
    "class WriteHead(Head):\n",
    "    def __init__(self, memory: Memory, hidden_size: int) -> None:\n",
    "        super(WriteHead, self).__init__(memory=memory, hidden_size=hidden_size)\n",
    "        self.e_layer = nn.Linear(hidden_size, memory.size[1])\n",
    "        self.a_layer = nn.Linear(hidden_size, memory.size[1])\n",
    "\n",
    "        for layer in [self.e_layer, self.a_layer]:\n",
    "            nn.init.xavier_uniform_(layer.weight, gain=1.4)\n",
    "            nn.init.normal_(layer.bias, std=0.01)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, previous_state: torch.Tensor) -> torch.Tensor:\n",
    "        w = self.get_head_weight(x, previous_state, self.memory.matrix)\n",
    "        e = torch.sigmoid(self.e_layer(x))\n",
    "        a = self.a_layer(x)\n",
    "\n",
    "        self.memory.write(w, e, a)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T06:16:40.388022Z",
     "iopub.status.busy": "2022-07-03T06:16:40.387779Z",
     "iopub.status.idle": "2022-07-03T06:16:40.397609Z",
     "shell.execute_reply": "2022-07-03T06:16:40.396786Z"
    },
    "id": "QLyw7HFeecon"
   },
   "outputs": [],
   "source": [
    "class LSTMController(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int) -> None:\n",
    "        super(LSTMController, self).__init__()\n",
    "        self.layer = nn.LSTM(input_size=input_size, hidden_size=hidden_size)\n",
    "        self.lstm_h_state = nn.Parameter(torch.randn(1, 1, hidden_size) * 0.05)\n",
    "        self.lstm_c_state = nn.Parameter(torch.randn(1, 1, hidden_size) * 0.05)\n",
    "\n",
    "        for p in self.layer.parameters():\n",
    "            if p.dim() == 1:\n",
    "                nn.init.constant_(p, 0)\n",
    "            else:\n",
    "                stdev = 5 / (np.sqrt(input_size + hidden_size))\n",
    "                nn.init.uniform_(p, -stdev, stdev)\n",
    "\n",
    "    def get_initial_state(self, batch_size: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        lstm_h = self.lstm_h_state.clone().repeat(1, batch_size, 1)\n",
    "        lstm_c = self.lstm_c_state.clone().repeat(1, batch_size, 1)\n",
    "        return lstm_h, lstm_c\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, state: Tuple[torch.Tensor]\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor]]:\n",
    "        output, state = self.layer(x.unsqueeze(0), state)\n",
    "        return output.squeeze(0), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T06:16:40.401505Z",
     "iopub.status.busy": "2022-07-03T06:16:40.401157Z",
     "iopub.status.idle": "2022-07-03T06:16:40.415116Z",
     "shell.execute_reply": "2022-07-03T06:16:40.414413Z"
    },
    "id": "wnpN72S3eetl"
   },
   "outputs": [],
   "source": [
    "class NTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_ways: int,\n",
    "        img_size: int = 28,\n",
    "        memory_size: Tuple[int, int] = (128, 40),\n",
    "        hidden_size: int = 200,\n",
    "    ) -> None:\n",
    "        super(NTM, self).__init__()\n",
    "        input_size = img_size * img_size + num_ways\n",
    "        controller_input_size = input_size + memory_size[1]\n",
    "\n",
    "        self.memory = Memory(size=memory_size)\n",
    "        self.read_head = ReadHead(memory=self.memory, hidden_size=hidden_size)\n",
    "        self.write_head = WriteHead(memory=self.memory, hidden_size=hidden_size)\n",
    "        self.controller = LSTMController(\n",
    "            input_size=controller_input_size, hidden_size=hidden_size\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size + memory_size[1], 5)\n",
    "        nn.init.xavier_uniform_(self.fc.weight, gain=1)\n",
    "        nn.init.normal_(self.fc.bias, std=0.01)\n",
    "\n",
    "    def get_initial_state(\n",
    "        self, batch_size: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Tuple[torch.Tensor]]:\n",
    "        self.memory.reset(batch_size)\n",
    "        read = self.memory.get_initial_read(batch_size)\n",
    "        read_head_state = self.read_head.get_initial_state(batch_size)\n",
    "        write_head_state = self.write_head.get_initial_state(batch_size)\n",
    "        controller_state = self.controller.get_initial_state(batch_size)\n",
    "        return (read, read_head_state, write_head_state, controller_state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        previous_state: Tuple[\n",
    "            torch.Tensor, torch.Tensor, torch.Tensor, Tuple[torch.Tensor]\n",
    "        ],\n",
    "    ) -> Tuple[\n",
    "        torch.Tensor,\n",
    "        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Tuple[torch.Tensor]],\n",
    "    ]:\n",
    "        (\n",
    "            previous_read,\n",
    "            previous_read_head_state,\n",
    "            previous_write_head_state,\n",
    "            previous_controller_state,\n",
    "        ) = previous_state\n",
    "\n",
    "        controller_input = torch.cat([x, previous_read], dim=1)\n",
    "        controller_output, controller_state = self.controller(\n",
    "            controller_input, previous_controller_state\n",
    "        )\n",
    "        read_head_output, read_head_state = self.read_head(\n",
    "            controller_output, previous_read_head_state\n",
    "        )\n",
    "        write_head_state = self.write_head(controller_output, previous_write_head_state)\n",
    "\n",
    "        fc_input = torch.cat((controller_output, read_head_output), dim=1)\n",
    "        state = (read_head_output, read_head_state, write_head_state, controller_state)\n",
    "        return F.softmax(self.fc(fc_input), dim=1), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T06:16:40.418962Z",
     "iopub.status.busy": "2022-07-03T06:16:40.418795Z",
     "iopub.status.idle": "2022-07-03T06:16:40.428691Z",
     "shell.execute_reply": "2022-07-03T06:16:40.427880Z"
    },
    "id": "46DqD9KVexiw"
   },
   "outputs": [],
   "source": [
    "def generate_sequence(\n",
    "    xs: torch.Tensor, ys: torch.Tensor, num_ways: int, device: str\n",
    ") -> torch.Tensor:\n",
    "    xs_flat = xs.flatten(2, 4)\n",
    "\n",
    "    ys_onehot = F.one_hot(ys, num_classes=num_ways)\n",
    "    ys_cat = torch.cat(\n",
    "        (\n",
    "            torch.zeros(ys_onehot.shape[0], 1, ys_onehot.shape[2]).to(device=device),\n",
    "            ys_onehot,\n",
    "        ),\n",
    "        dim=1,\n",
    "    )[:, :-1, :]\n",
    "\n",
    "    seq = torch.cat((xs_flat, ys_cat), dim=2)\n",
    "    return torch.swapaxes(seq, 0, 1)\n",
    "\n",
    "\n",
    "def generate_sequence_v2(\n",
    "    task_batch: Dict[str, List[torch.Tensor]], device: str, num_ways: int\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    support_xs = task_batch[\"train\"][0].to(device=device)\n",
    "    support_ys = task_batch[\"train\"][1].to(device=device)\n",
    "    query_xs = task_batch[\"test\"][0].to(device=device)\n",
    "    query_ys = task_batch[\"test\"][1].to(device=device)\n",
    "\n",
    "    random_indices = torch.randperm(5)\n",
    "    query_xs_ = query_xs[:, random_indices, :, :, :]\n",
    "    query_ys_ = query_ys[:, random_indices]\n",
    "\n",
    "    support_seq = generate_sequence(\n",
    "        xs=support_xs, ys=support_ys, num_ways=num_ways, device=device\n",
    "    )\n",
    "    query_seq = generate_sequence(\n",
    "        xs=query_xs_, ys=query_ys_, num_ways=num_ways, device=device\n",
    "    )\n",
    "\n",
    "    x_seq = torch.cat((support_seq, query_seq), dim=0)\n",
    "    y_seq = torch.cat((support_ys, query_ys_), dim=1)\n",
    "    return x_seq, y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T06:16:40.432663Z",
     "iopub.status.busy": "2022-07-03T06:16:40.432401Z",
     "iopub.status.idle": "2022-07-03T06:16:40.442223Z",
     "shell.execute_reply": "2022-07-03T06:16:40.441408Z"
    },
    "id": "JzHdwtJad2cn"
   },
   "outputs": [],
   "source": [
    "def train_mann(\n",
    "    num_ways: int,\n",
    "    num_shots: int,\n",
    "    task_batch_size: int,\n",
    "    device: str,\n",
    "    task_batch: Dict[str, List[torch.Tensor]],\n",
    "    model: NTM,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    "    optimizer: torch.optim.RMSprop,\n",
    ") -> Tuple[float, float]:\n",
    "    model.train()\n",
    "\n",
    "    x_seq_, y_seq_ = generate_sequence_v2(\n",
    "        task_batch=task_batch, device=device, num_ways=num_ways\n",
    "    )\n",
    "    x_seq = x_seq_.to(device=device)\n",
    "    y_seq = y_seq_.to(device=device)\n",
    "\n",
    "    state = model.get_initial_state(batch_size=task_batch_size)\n",
    "    prob = torch.zeros((len(x_seq), task_batch_size, num_ways)).to(device=device)\n",
    "    for j, vector in enumerate(x_seq):\n",
    "        prob[j], state = model(vector, state)\n",
    "    prob_ = prob.permute(1, 2, 0)\n",
    "    loss = criterion(prob_, y_seq)\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        num_query = num_ways * num_shots\n",
    "        correct = torch.sum(\n",
    "            prob_[..., -num_query:].argmax(dim=1) == y_seq[..., -num_query:]\n",
    "        )\n",
    "        accuracy = correct.item() / np.prod(y_seq[..., -num_query:].size())\n",
    "    return accuracy, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T06:16:40.445306Z",
     "iopub.status.busy": "2022-07-03T06:16:40.445066Z",
     "iopub.status.idle": "2022-07-03T06:16:40.454344Z",
     "shell.execute_reply": "2022-07-03T06:16:40.453531Z"
    },
    "id": "NCDFQSd3d4Pc"
   },
   "outputs": [],
   "source": [
    "def test_mann(\n",
    "    num_ways: int,\n",
    "    num_shots: int,\n",
    "    task_batch_size: int,\n",
    "    device: str,\n",
    "    task_batch: Dict[str, List[torch.Tensor]],\n",
    "    model: NTM,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    ") -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "\n",
    "    x_seq_, y_seq_ = generate_sequence_v2(\n",
    "        task_batch=task_batch, device=device, num_ways=num_ways\n",
    "    )\n",
    "    x_seq = x_seq_.to(device=device)\n",
    "    y_seq = y_seq_.to(device=device)\n",
    "\n",
    "    state = model.get_initial_state(task_batch_size)\n",
    "    prob = torch.zeros((len(x_seq), task_batch_size, num_ways)).to(device=device)\n",
    "    for i, vector in enumerate(x_seq):\n",
    "        prob[i], state = model(vector, state)\n",
    "    prob_ = prob.permute(1, 2, 0)\n",
    "    loss = criterion(prob_, y_seq)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        num_query = num_ways * num_shots\n",
    "        correct = torch.sum(\n",
    "            prob_[..., -num_query:].argmax(dim=1) == y_seq[..., -num_query:]\n",
    "        )\n",
    "        accuracy = correct.item() / np.prod(y_seq[..., -num_query:].size())\n",
    "    return accuracy, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T06:16:40.458071Z",
     "iopub.status.busy": "2022-07-03T06:16:40.457828Z",
     "iopub.status.idle": "2022-07-03T06:16:40.464379Z",
     "shell.execute_reply": "2022-07-03T06:16:40.463574Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(output_folder: str, model: NTM, title: str) -> None:\n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "    filename = os.path.join(output_folder, title)\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        state_dict = model.state_dict()\n",
    "        torch.save(state_dict, f)\n",
    "    print(\"Model is saved in\", filename)\n",
    "\n",
    "\n",
    "def load_model(output_folder: str, model: NTM, title: str) -> None:\n",
    "    filename = os.path.join(output_folder, title)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    print(\"Model is loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T06:16:40.467513Z",
     "iopub.status.busy": "2022-07-03T06:16:40.467172Z",
     "iopub.status.idle": "2022-07-03T06:16:40.473983Z",
     "shell.execute_reply": "2022-07-03T06:16:40.473172Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_graph(\n",
    "    train_accuracies: List[float],\n",
    "    val_accuracies: List[float],\n",
    "    train_losses: List[float],\n",
    "    val_losses: List[float],\n",
    ") -> None:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    axs[0].plot(train_accuracies, label=\"train_acc\")\n",
    "    axs[0].plot(val_accuracies, label=\"test_acc\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(train_losses, label=\"train_loss\")\n",
    "    axs[1].plot(val_losses, label=\"test_loss\")\n",
    "    axs[1].set_title(\"Loss\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T06:16:40.476956Z",
     "iopub.status.busy": "2022-07-03T06:16:40.476714Z",
     "iopub.status.idle": "2022-07-03T06:16:43.468901Z",
     "shell.execute_reply": "2022-07-03T06:16:43.467802Z"
    },
    "id": "CcoXVDCnd5fb"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"folder_name\": \"../load_dataset/dataset\",\n",
    "    \"download\": False,\n",
    "    \"num_shots\": 1,\n",
    "    \"num_ways\": 5,\n",
    "    \"output_folder\": \"saved_model\",\n",
    "    \"task_batch_size\": 32,  # 필수\n",
    "    \"num_task_batch_train\": 100000,  # 필수\n",
    "    \"num_task_batch_test\": 30000,  # 필수\n",
    "    \"device\": \"cuda\",  # 필수\n",
    "}\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = get_dataloader(config)\n",
    "\n",
    "model = NTM(num_ways=config[\"num_ways\"]).to(device=config[\"device\"])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), momentum=0.9, alpha=0.95, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T06:16:43.473607Z",
     "iopub.status.busy": "2022-07-03T06:16:43.473341Z",
     "iopub.status.idle": "2022-07-03T16:44:47.715033Z",
     "shell.execute_reply": "2022-07-03T16:44:47.714034Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/100000 [00:15<60:28:30,  2.18s/it, train_accuracy=0.2250, train_loss=1.6075, val_accuracy=0.2250, val_loss=1.6059]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 22\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     12\u001b[0m train_accuracy, train_loss \u001b[39m=\u001b[39m train_mann(\n\u001b[0;32m     13\u001b[0m     num_ways\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mnum_ways\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     14\u001b[0m     num_shots\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mnum_shots\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 22\u001b[0m val_accuracy, val_loss \u001b[39m=\u001b[39m test_mann(\n\u001b[0;32m     23\u001b[0m     num_ways\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_ways\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     24\u001b[0m     num_shots\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_shots\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     25\u001b[0m     task_batch_size\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mtask_batch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     26\u001b[0m     device\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mdevice\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     27\u001b[0m     task_batch\u001b[39m=\u001b[39;49mval_batch,\n\u001b[0;32m     28\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     29\u001b[0m     criterion\u001b[39m=\u001b[39;49mcriterion,\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     32\u001b[0m train_accuracies\u001b[39m.\u001b[39mappend(train_accuracy)\n\u001b[0;32m     33\u001b[0m val_accuracies\u001b[39m.\u001b[39mappend(val_accuracy)\n",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m, in \u001b[0;36mtest_mann\u001b[1;34m(num_ways, num_shots, task_batch_size, device, task_batch, model, criterion)\u001b[0m\n\u001b[0;32m     19\u001b[0m prob \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(x_seq), task_batch_size, num_ways))\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m i, vector \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(x_seq):\n\u001b[1;32m---> 21\u001b[0m     prob[i], state \u001b[39m=\u001b[39m model(vector, state)\n\u001b[0;32m     22\u001b[0m prob_ \u001b[39m=\u001b[39m prob\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[0;32m     23\u001b[0m loss \u001b[39m=\u001b[39m criterion(prob_, y_seq)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\meta\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[9], line 55\u001b[0m, in \u001b[0;36mNTM.forward\u001b[1;34m(self, x, previous_state)\u001b[0m\n\u001b[0;32m     51\u001b[0m controller_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x, previous_read], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     52\u001b[0m controller_output, controller_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontroller(\n\u001b[0;32m     53\u001b[0m     controller_input, previous_controller_state\n\u001b[0;32m     54\u001b[0m )\n\u001b[1;32m---> 55\u001b[0m read_head_output, read_head_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_head(\n\u001b[0;32m     56\u001b[0m     controller_output, previous_read_head_state\n\u001b[0;32m     57\u001b[0m )\n\u001b[0;32m     58\u001b[0m write_head_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_head(controller_output, previous_write_head_state)\n\u001b[0;32m     60\u001b[0m fc_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((controller_output, read_head_output), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\meta\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m, in \u001b[0;36mReadHead.forward\u001b[1;34m(self, x, previous_state)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m      3\u001b[0m     \u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor, previous_state: torch\u001b[39m.\u001b[39mTensor\n\u001b[0;32m      4\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m----> 5\u001b[0m     w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_head_weight(x, previous_state, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49mmatrix)\n\u001b[0;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mmatmul(w\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mmatrix)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m), w\n",
      "Cell \u001b[1;32mIn[5], line 42\u001b[0m, in \u001b[0;36mHead.get_head_weight\u001b[1;34m(self, x, previous_state, memory_matrix)\u001b[0m\n\u001b[0;32m     33\u001b[0m gamma \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m F\u001b[39m.\u001b[39msoftplus(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma_layer(x))\n\u001b[0;32m     35\u001b[0m w_c \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(\n\u001b[0;32m     36\u001b[0m     beta\n\u001b[0;32m     37\u001b[0m     \u001b[39m*\u001b[39m F\u001b[39m.\u001b[39mcosine_similarity(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m     41\u001b[0m )\n\u001b[1;32m---> 42\u001b[0m w_g \u001b[39m=\u001b[39m g \u001b[39m*\u001b[39m w_c \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m g) \u001b[39m*\u001b[39;49m previous_state\n\u001b[0;32m     43\u001b[0m w_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shift(w_g, s)\n\u001b[0;32m     45\u001b[0m w \u001b[39m=\u001b[39m w_t\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgamma\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 메타-트레이닝\n",
    "with tqdm(\n",
    "    zip(train_dataloader, val_dataloader), total=config[\"num_task_batch_train\"]\n",
    ") as pbar:\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for task_batch_idx, (train_batch, val_batch) in enumerate(pbar):\n",
    "        if task_batch_idx >= config[\"num_task_batch_train\"]:\n",
    "            break\n",
    "\n",
    "        train_accuracy, train_loss = train_mann(\n",
    "            num_ways=config[\"num_ways\"],\n",
    "            num_shots=config[\"num_shots\"],\n",
    "            task_batch_size=config[\"task_batch_size\"],\n",
    "            device=config[\"device\"],\n",
    "            task_batch=train_batch,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "        )\n",
    "        val_accuracy, val_loss = test_mann(\n",
    "            num_ways=config[\"num_ways\"],\n",
    "            num_shots=config[\"num_shots\"],\n",
    "            task_batch_size=config[\"task_batch_size\"],\n",
    "            device=config[\"device\"],\n",
    "            task_batch=val_batch,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "        )\n",
    "\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            train_accuracy=\"{0:.4f}\".format(train_accuracy),\n",
    "            val_accuracy=\"{0:.4f}\".format(val_accuracy),\n",
    "            train_loss=\"{0:.4f}\".format(train_loss),\n",
    "            val_loss=\"{0:.4f}\".format(val_loss),\n",
    "        )\n",
    "\n",
    "    # 모델 저장하기\n",
    "    save_model(output_folder=config[\"output_folder\"], model=model, title=\"mann.th\")\n",
    "\n",
    "    print_graph(\n",
    "        train_accuracies=train_accuracies,\n",
    "        val_accuracies=val_accuracies,\n",
    "        train_losses=train_losses,\n",
    "        val_losses=val_losses,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-07-03T16:44:47.719821Z",
     "iopub.status.busy": "2022-07-03T16:44:47.719637Z",
     "iopub.status.idle": "2022-07-03T17:47:21.252041Z",
     "shell.execute_reply": "2022-07-03T17:47:21.250020Z"
    },
    "executionInfo": {
     "elapsed": 57197,
     "status": "ok",
     "timestamp": 1642232824410,
     "user": {
      "displayName": "Luna Jang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiBq_mQ3pvg1aATbjbemG_YdHPPMMsRmQcVaghkug=s64",
      "userId": "06164029183671863730"
     },
     "user_tz": -540
    },
    "id": "LXDHudwid9kV",
    "outputId": "f9bbfabe-eb26-4d7b-e35b-af6835db0399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 30000/30000 [1:02:33<00:00,  7.99it/s, test_accuracy=0.7792, test_loss=1.2523]\n"
     ]
    }
   ],
   "source": [
    "# 모델 불러오기\n",
    "load_model(output_folder=config[\"output_folder\"], model=model, title=\"mann.th\")\n",
    "\n",
    "# 메타-테스팅\n",
    "with tqdm(test_dataloader, total=config[\"num_task_batch_test\"]) as pbar:\n",
    "    sum_test_accuracies = 0.0\n",
    "    sum_test_losses = 0.0\n",
    "\n",
    "    for task_batch_idx, test_batch in enumerate(pbar):\n",
    "        if task_batch_idx >= config[\"num_task_batch_test\"]:\n",
    "            break\n",
    "\n",
    "        test_accuracy, test_loss = test_mann(\n",
    "            num_ways=config[\"num_ways\"],\n",
    "            num_shots=config[\"num_shots\"],\n",
    "            task_batch_size=config[\"task_batch_size\"],\n",
    "            device=config[\"device\"],\n",
    "            task_batch=test_batch,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "        )\n",
    "\n",
    "        sum_test_accuracies += test_accuracy\n",
    "        sum_test_losses += test_loss\n",
    "        pbar.set_postfix(\n",
    "            test_accuracy=\"{0:.4f}\".format(sum_test_accuracies / (task_batch_idx + 1)),\n",
    "            test_loss=\"{0:.4f}\".format(sum_test_losses / (task_batch_idx + 1)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPruUyaYfknRac+TH+CMFnl",
   "collapsed_sections": [],
   "name": "2.2.3.MANN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "meta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "543a2858b738eacd0ac3a1925ec6f5fb4c9073862b17acbfaa542a46096ac3bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
